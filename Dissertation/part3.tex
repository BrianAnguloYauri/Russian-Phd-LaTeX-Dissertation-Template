\chapter{Метод для решения для задачи автономной навигации на основе обучения с подкреплением}\label{ch:ch3}

% Автономная навигация быстро развивается и в то же время требует эффективного планирования движения в сложных и высокодинамичных средах учитывая при этом 
% кинодинамические ограничения неголономного автономного транспортного средства. Часто алгоритмы планирования, рассматривающие 
% первый аспект проблемы, то есть высокодинамичную среду, подобные тем, которые представлены в~\cite{Otte2016RRTX, phillips2011sipp}, 
% не рассматривают кинодинамические ограничения робота. С другой стороны, специалисты по кинодинамическому планированию часто 
% не рассуждают явно о будущих изменениях окружающей среды, даже если эти изменения предвидимы, например, прогнозируется системой технического зрения. 
% В данной главе мы предлагаем решить эту проблему обогатив методы кинодинамического планирования возможностью учитывать и 
% динамику окружающей среды (на этапе планирования).


% \section{Предлагаемый метод/ abstracts}\label{sec:ch3/sec2}

% В этой работе мы предлагаем алгоритм оптимизации стратегии, предназначенной для обучения адаптивных примитивов движения, 
% позволяющий учитывать будущие изменения окружающей среды на этапе планирования и при этом создавать планы, удовлетворяющие кинодинамическим 
% ограничениям робота. POLAMP использует подход на основе обучения с подкреплением, чтобы найти стратегию, которая генерирует локальные сегменты траектории, 
% встроенные в алгоритмы глобального планирования, RRT и A*, для создания глобального плана движения. Наш обучаемый локальный планировщик использует 
% частичное наблюдение, чтобы избегать столкновение как со статическими, так и динамическими препятствиями, учитывая кинодинамические ограничения робота. 
% В результате POLAMP способен генерировать осуществимые решения с высокой вероятностью успеха ($>92\%$) в средах с движущимися препятствиями вплоть 
% до 50, что превосходит резултаты конкурентов.

% \section{Введение}

% % Автономные роботизированные системы стали одной из самых популярных тем исследований в последние годы из-за огромных потенциальных социальных преимуществ. 
% % В частности, автономное вождение быстро развивается и в то же время требует эффективного планирования движения в сложных и высокодинамичных средах 
% % с учетом кинодинамических ограничений неголономного автономного транспортного средства. Часто планировщики, рассматривающие первый аспект проблемы, 
% % то есть динамическую среду, подобные тем, которые представлены в~\cite{Otte2016RRTX, phillips2011sipp}, не принимают во внимание кинодинамические 
% % ограничения. С другой стороны, специалисты по кинодинамическому планированию часто не рассуждают явно о будущих изменениях окружающей среды, даже 
% % если эти изменения предвидимы, например. прогнозируется системой управления робота. В данной работе мы хотим обогатить методы кинодинамического 
% % планирования возможностью учитывать и динамику окружающей среды (на этапе планирования).

% \subsection{Кинодинамическое планирование}

% Для решения кинодинамического планирования имеются два распространенных подхода: методы планирования на основе построения решеток
% и методы планирования на основе случайного сэмплирования. Методы планирования на основе решетки используют так называемые примитивы 
% движения~\cite{MotionPrimitives}, которые образуют регулярную решетку. Каждый примитив движения представляет собой 
% небольшой сегмент кинодинамически осуществимой траектории робота, который предварительно рассчитывается перед планированием. 
% На этапе планирования алгоритмы поиска (например, A*~\cite{hart1968formal} его вариантов) используются для поиска результирующей 
% траектории, представленной как последовательность примитивов движения. Напротив, планировщики на основе случайного сэмплирования, например 
% RRT~\cite{RRT} или RRT*~\cite{RRT*}, растут дерево поиска путем сэмплирования состояний в пространстве конфигурации робота и применяет
% локальный планировщик для соединения двух состояний, соблюдая при этом кинематические ограничения робота. 
% Таким образом, примитивы движения создаются онлайн (т.е. во время планирования).

% Одним из известных подходов, который уменьшает сложность локального планирования с учетом кинематических ограничений робота является 
% использование методов, основанных на обучении с подкреплением. Такие методы были предложены в RL-RRT~\cite{Chiang2019RL-RRT} и 
% PRM-RL~\. cite{Faust2018PRM-RL}.

% \begin{figure}[]
%     \centering
%     \includegraphics[width=0.49\textwidth]{Images/IntroFigure.png}
%     \caption{Illustration of the POLAMP algorithm. \textbf{Red} arrow represent the start state, \textbf{Blue} arrow -- the goal one. \textbf{Black} and \textbf{Blue} rectangles represent the static and dynamic obstacles respectively. \textbf{Cyan} curve is the generated trajectory by A*-POLAMP.}\label{fig:IntroFigure}
% \end{figure}

% \section{Related Work}


% Проблема кинодинамического планирования хорошо изучена и имеются различные подходы, основанные на решетках, случайном сэмплировании, оптимизации, 
% обучении с подкреплением или их комбинация, обзор см. в ~\cite{gonzalez2015review}. Тем не менее, кинодинамическое планирование при наличии динамических 
% препятствий по-прежнему остается сложной задачей.

% Распространенным подходом к кинодинамическому планированию в робототехнике являются планировщики на основе сэмплирования. Самый популярный способ 
% учета динамики робота — это сэмплирование в пространстве состояний робота и попытка соединить состояния с помощью различных локальных 
% планировщиков~\cite{hwan2011anytime,webb2013kinodynamic,BIT*SQP,Otte2016RRTX}, в том числе для робота, похожего на 
% автомобиль~\cite{vailland2021CubicBezier}. Метод, описанный в этой работе, также основан на локальном планировщике, но он обучаем и 
% учитывает движущиеся препятствия. В отличие от методов, представленных в~\cite{Otte2016RRTX, Chen2019Horizon}, он предполагает, что информация 
% о том, как препятствия должны двигаться в будущем, доступна (например, прогнозируется на основе наблюдений датчиков), и учитывает эту информацию 
% при планировании.

% Недавно был предложен планировщик на основе решеток для роботов, похожих на автомобили, в высокодинамичных средах~\cite{lin2021search}. 
% Другие варианты таких планировщиков для роботов, похожих на автомобили, описаны в ~\cite{MotionPrimitives, rufli2010design, ziegler2009spatiotemporal}. 
% В отличие от этих алгоритмов предлагаемый метод не строит решетку в многомерном пространстве для поиска допустимого плана, а 
% использует локальный обучаемый планировщик для соединения состояний.

% Также существуют методы, которые сначала генерируют приблизительный путь, часто такой, который не учитывает кинодинамические ограничения, 
% а затем генерируют элементы управления для следования по пути, соблюдая динамику системы и избегая препятствий. 
% Варианты этих методов описаны в~\cite{perez2021robot, kontoudis2019kinodynamic}. 
% В отличие от них предложенный в данной работе метод строит допустимую траекторию за один шаг планирования. 
% Уклонение от движущихся препятствий осуществляется за счет знания их будущих траекторий.
% Есть и другие работы, посвященные планированию кинодинамического движения с использованием архитектуры «Актер-Критик» с совершенно 
% неизвестной динамической системой. В отличие от этих методов, вместо того, чтобы следовать первоначальным предположениям, 
% генерирующим кинодинамические управления, мы в первую очередь строим кинодинамический путь между заданными состояниями.

% Наконец, наиболее похожими на представленный в этой статье методами являются RL-RRT~\cite{Chiang2019RL-RRT} и PRM-RL~\cite{Faust2018PRM-RL}. 
% Наш метод также использует обучающийся локальный планировщик внутри планировщика на основе случайного сэмплирования. Однако, в отличие от этих методов, 
% наш местный планировщик учитывает наличие динамических препятствий.

% \section{Задача навигация в динамической среде}\label{sec:ch3/sec1}

% Уточним постановку задачи навигации из главы 1. В этой главе мы заинтересованы в планировании допустимой кинодинамической траектории для неголомонного 
% робота, которая избегает столкновения как со статическими, так и движущимися препятствями. В частности, нас интересуют роботы с динамикой автомобиля, 
% которые описываются как в ~\cite{surveyMotionPlanning}:

% \begin{align}\label{eq:diffEquationsRobot}
%      &\dot{x} = v cos(\theta)\nonumber\\
%      &\dot{y} = v sin(\theta)\\
%      &\dot{\theta} = \frac{v}{L} \tan(\gamma), \nonumber
% \end{align}
% где $x$,$y$ — координаты опорной точки робота (середина задней оси), $\theta$ — ориентация, $L$ — колёсная база, $v$ — линейная скорость, 
% $ \gamma$ — угол поворота. Первые три переменные составляют вектор состояния: $\boldsymbol{x}(t)=(x,y,\theta)$.
% Последние две переменные образуют вектор управления: $\boldsymbol{u}(t) = (v, \gamma)$, который также можно переписать с помощью 
% ускорения $a$ и скорости вращения $\omega$ следующим образом : $v = v_0 + a \cdot t, \gamma = \gamma_0 + \omega \cdot t$.

% Робот работает в 2D-рабочем пространстве, заполненном статическими и динамическими препятствиями. Их форма прямоугольная (как у робота). 
% Пусть $Obs = \{ Obs_1(t), ..., Obs_n(t)\}$ обозначает набор препятствий, где $Obs_i(t)$ отображает моменты времени в положения опорной точки 
% препятствия в рабочем пространстве. . Для статических препятствий, очевидно, справедливо соотношение $\forall t:Obs_i(t)=Obs_i(0)$. В нашей 
% работе мы считаем функции $Obs_i(t)$ известными.

% Обозначим через $\mathcal{X}_{free}(t)$ все конфигурации робота, которые не сталкиваются ни с одним из препятствий в момент времени $t$ (
% с учетом формы робота и препятствий). Теперь проблема состоит в том, чтобы найти элементы управления (как функции времени), которые 
% переводят робота из начальной конфигурации $s_{start}$ в целевую $s_{goal}$ s.t. что кинодинамические ограничения~(\ref{eq:diffEquationsRobot}) 
% соблюдены и результирующая траектория лежит в $\mathcal{X}_{free}(t)$.


% \subsection{Задача навигации в среде POLAMP}


% \section{Метод}

% Для решения описанной проблемы мы используем сочетание глобального и локального планирования. Глобальный планировщик нацелен на 
% систематическое разложение проблемы на набор подзадач, которые легче решить, т.е. переход от одной конфигурации к другой. Локальный планировщик 
% предназначен для решения последней проблемы. Любая такая подзадача по сути представляет собой задачу двух краев с дополнительными ограничениями 
% (запрещающими роботу сталкиваться как со статическими, так и с динамическими препятствиями), которую трудно решить напрямую. Суть нашего подхода состоит 
% в том, чтобы представить эту проблему как частично наблюдаемый марковский процесс принятия решений (POMDP) и получить стратегию для решения POMDP посредством 
% обучения с подкреплением, а точнее, с помощью специально разработанного алгоритма PPO. Как только стратегия получена 
% (обучена), мы подключаем ее к глобальному планировщику. Как глобальный планировщик мы можем использовать адаптацию известных алгоритмов RRT и A*, 
% чтобы получить окончательный решатель. Мы называем этот тип решателей POLAMP — оптимизация стратегии для изучения примитивов адаптивного движения.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.4\textwidth]{Images/NewActorCriticScheme.png}
%     \caption{Actor-Critic architecture that is implemented in POLAMP}
%     \label{fig:actor-critic-arch}
% \end{figure}

% \subsection{Обучаемый локальный планировщик}

% \paragraph{Background}
% Формально POMDP можно представить как кортеж $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \Omega)$, 
% где $\mathcal{S}$ — это пространство состояний, $\mathcal{A}$ — пространство действий, $\mathcal{P}$ — модель перехода состояний, 
% $\mathcal{R}$ — функция вознаграждения, $\Omega$ — пространство наблюдения. В ходе обучения на каждом временном шаге агент получает наблюдение 
% $o_t \in \Omega$, совершает действие $a_t \in \mathcal{A}$ и получает вознаграждение $r_t \in \mathcal{R}$. Цель состоит в том, чтобы обучить стратегию, 
% то есть сопоставление наблюдений с распределениями действий, $\pi: \Omega\rightarrow P(\mathcal A)$. Стратегия должна максимизировать следующую ожидаемую 
% дисконтированную награду от начального состояния $s_t$:

% \begin{equation*}
%    J(\pi) = \mathbb E_{r_{i}, s_{i} \sim \mathcal{P}, a_{i} \sim \pi}[\sum_{i=t}^T \gamma^{i-t}r(s_i, a_i) | s_t, a_t], i>t,
% \end{equation*}
% где $\gamma$ — дисконтирующий коэффициент.

% $Q$-функция используется для краткого определения наиболее важной информации для агента для принятия оптимального решения:
% \begin{equation*}
%     Q^{\pi}(s_t, a_t) = \mathbb{E}_{r_{i}, s_{i} \sim \mathcal{P}, a_{} \sim \pi}[R_t | s_t, a_t], i>t,
% \end{equation*}

% % \begin{figure}[t]
% %     \centering
% %     \includegraphics[width=0.35\textwidth]{Images/learningEnvironment.png}
% %     \caption{The learning environment. \textbf{Green} rectangle with the \textbf{Red} arrow is the current state of the robot. \textbf{Green} rectangle with the \textbf{cyan} orientation is the goal desired state. \textbf{Blue} rectangles are the static obstacles and the \textbf{Blue} rectangle with \textbf{Pink} arrow is the moving obstacle. \textbf{Orange} lines are the laser beams.}
% %     \label{fig:learningEnvironment}
% % \end{figure}

% В данной статье мы рассматриваем алгоритмы семейства актор-критик, которые более стабильны, имеют меньшую дисперсию и менее склонны к сходимости к 
% локальному минимуму. Актор обновляет аппроксиматор стратегии $\hat \pi_w$, используя следующее уравнение:

% \begin{equation*}
%     \nabla_w J(\hat{\pi}_w) = \mathbb{E}_{\hat{\pi}_w} \big[ \nabla_w \log \hat{\pi}_w(s,a) Q^{\hat{\pi}_w}(s,a) \big],
% \end{equation*}
% где $\hat \pi_w$ — произвольная дифференцируемая стратегия. Критик оценивает аппроксимацию значения $Q^{\hat{\pi}_w}(s,a)$ для текущей стратегии $\hat \pi_w$. 
% Алгоритмы актор-критик имеют два набора параметров: критик обновляет параметры $\phi$ $Q$-функции, а актор обновляет параметры $w$ стратегии в 
% соответствии с предположениями критика.

% В этой работе мы используем метод оптимизации проксимальной стратегии~\cite{PPO} (PPO), поскольку он показал лучшую производительность среди других 
% методов в нашей предварительной оценке. Актерская часть PPO оптимизирует функцию ограниченных потерь.
% \begin{multline*}
%     L(s, a, w_k, w) = \min (\frac{\pi_w(a|s)}{\pi_{w_k}(a|s)} A^{\pi_{w_{old}}}(s, a), \\
%     clip(\frac{\pi_w(a|s)}{\pi_{w_{old}}(a|s)}, 1- \epsilon, 1+\epsilon) A^{\pi_{w_{old}}}(s, a)),
% \end{multline*}
% где $A^{\pi_{w}}$ — оценка функции преимущества $A(s, a) = Q(s, a) - V(s)$, заданная критикой. Отсечение – это регуляризатор, 
% устраняющий стимулы для резкого изменения стратегии. Гиперпараметр $\epsilon$ соответствует тому, насколько далеко новая стратегия 
% может уйти от старой, сохраняя при этом прибыль от цели. При интеграции алгоритма PPO в наш метод мы рассматривали состояние $s_t$ как функцию 
% наблюдения $s_t\approx f(o_t)$, где $f$ — нижние слои нейросетевого аппроксиматора актера и критика, показанного на рис.

% \subsubsection{Наблюдения, действия и награда}

% \paragraph{Наблюдения, действия и награды\label{localPlannner}}\label{paragraph:observations-actions-reward}

% В данной работе рассматриваются действия $a_t = (a, \omega) \in R^2$, состоящие из задания линейного ускорения $a \in (-5, 5) \; m/s^2$ и 
% скорость вращения $\omega \in (-\pi/12, \pi/12) \;rad/s$. Последние можно преобразовать в элементы управления робота, используя преобразования 
% для уравнения~\ref{eq:diffEquationsRobot}, где мы задаем диапазон линейной скорости в $v \in (0, 4) \; m/s$ и угол поворота 
% в $\gamma\in (-\pi/6, \pi/6)\;rad$.

% Наблюдение $o_t$ представляет собой вектор, состоящий из $N_{beams}=39$ измерений лидара, охватывающих 360$^{\circ}$-окружение робота до 
% длины $beam_{max} = 20\;m$ -- см. рис.~\ref{fig:learningEnvironment}, объединенный с признаками 
% $(\Delta x, \Delta y, \Delta \theta, \Delta v, \Delta \gamma, \theta, v , \gamma, a, \omega)$, где $\Delta(s_i)$ — разница между соответствующим 
% параметром $s_i$ целевого состояния и текущим, $(\theta, v, \gamma)$ — последние три параметра текущего состояния, а $(a, \omega)$ — текущие элементы 
% управления. Мы рассматриваем идеальную среду, поэтому и симуляция, и модель срабатывания не имеют ошибок.

% Функция вознаграждения описывается следующим образом:
% \begin{equation*}
%      \mathcal R = w_r^T [r_{\text{цель}}, r_{\text{col}}, r_{\text{поле}}, r_{t}, r_{\text{backward}}, r_ {v_\text{max}}, r_{\gamma_\text{max}}],
% \end{equation*}

% где $w_r$ — вектор весов, $r_{\text{goal}}$ равен 1, если агент достиг целевого состояния с допуском $(\epsilon_{\rho}, \epsilon_{\theta})$ и 0 
% в противном случае, $r_{\text{col}}$ равно $-1$, если агент сталкивается с препятствиями, и 0 в противном случае, 
% $ r_{\text{field}} = \rho_{curr} - \rho_{last }$, где $\rho_{last} = \|s_{t - 1} - s_{goal}\|$ и $\rho_{curr} = \|s_t - s_{goal}\|$ мы
%  налагаем штраф на агента за отход от цели, $r_{t}=-1$ — постоянный штраф за каждый временной шаг, $r_{\text{backward}}$ — $-1$, агент использует 
%  заднюю передачу (движение назад ) и 0 в противном случае, $r_{v_\text{max}}$ равно $-1$ за превышение максимального ограничения скорости, 
%  $r_{\gamma_\text{max}}$ равно $-1$ за превышение максимального ограничения скорости порог угла поворота рулевого колеса. Мы устанавливаем 
%  веса $w_r = [20, 8, 1, 0,1, 0,3, 0,5, 0,5]$ (эмпирически эти значения приводят к более эффективному обучению).

% Для ускорения завершения обучения мы предлагаем трехступенчатую программу обучения (см. Рис.~\ref{fig:curriculum-training}). 
% На первом этапе мы обучаем агента в пустой среде. Этот этап предназначен для изучения кинодинамических ограничений транспортного средства. 
% Как только агент достигнет приемлемого уровня успешности (80\% решенных задач), прекращаем обучение и переходим к следующему этапу. 
% На втором этапе мы переобучаем стратегию в новой среде, наполненной статическими препятствиями, чтобы агент научился избегать столкновений с ними. 
% На последнем этапе мы добавляем враждебное динамическое препятствие в статическую среду, чтобы агент научился обходить его или ждать на месте, 
% если необходимо, чтобы препятствие исчезло. Последнее является важным навыком для планирования с динамическими препятствиями.

% \subsection{Глобальные методы планирования}

% % \begin{algorithm}[t]
% % \caption{POLAMP with RRT planner}
% % \label{alg:RRT-POLAMP}
% % \begin{algorithmic}[1]
% %     \Require $s_{start}$, $s_{goal}$, $Obs(t)$, $N_{max}$, RL-PI, $D$, $N_{nbs}$, $R_{ex}$
% %     \Ensure $\mathcal{P}$: Motion Plan
% %     %\Function{POLAMP-RRT}{$s_{start}$, $s_{goal}, Obs$, RL-PI}
% %     \State $s_{start}.t \gets 0$
% %     \State $\mathcal{T} \gets$
% %     \Call{InitializeTree}{$s_{start}$}
% %     \While{$N_{\max}$ was not reached}
% %         \State $s_{rand} \gets$ \Call{RandomSample}{}
% %         \State $neighbors \gets$ \Call{Nearest}{$\mathcal{T}, s_{rand}$, $N_{nbs}$}
% %         \For{$s_i \in neighbors $}
% %             \State $s_j \gets$ \Call{Extend}{$s_i, s_{rand}, R_{ex}$}
% %             %\If{$\neg$ \Call{Collides}{$s_{to}, Obs(t)$}}
% %             \State $s_j \gets$ \Call{RL-Steer}{$s_i, s_j, s_{goal}, Obs(t)$, RL-PI, $D$}
% %             %\State \textcolor{blue}{$\triangleright$ $s_{t_n}.t$ is the time at which the agent reach $s_{t_n}$}
% %             \If{$s_j.tr$ is not empty}
% %                 %\State \textcolor{blue}{$\triangleright$ the time $s_{t_n}.t$ is the current time of $s_{t_k}$ in A*}
% %                 \State $\mathcal{T} \gets$ \Call{APPEND}{$s_j$}
% %                 \If{$s_{goal}.tr$ is not empty}
% %                     \State $\mathcal{T} \gets$ \Call{APPEND}{$s_{goal}$}
% %                     \State \Return $\mathcal{P}$ = \Call{MotionPlan}{$\mathcal{T}$}
% %                 \Else
% %                      \State \textbf{break}    
% %                 \EndIf
% %             \EndIf
% %         \EndFor
% % %        \State $i \gets i + 1$
% %     \EndWhile
% %     \State \Return $\mathcal{P} = \emptyset$
% %     %\EndFunction
% % \end{algorithmic}
% % \end{algorithm}


% Хотя наш обучаемый локальный планировщик может генерировать траекторию движения между двумя соседними штатами, он не очень подходит для 
% построения долгосрочных планов. Поэтому мы предлагаем также использовать глобальный планировщик, который может последовательно исследовать 
% различные регионы рабочего пространства, опираясь на глобальное наблюдение, и находить пути достижения удаленных целей. В этой работе 
% мы используем классические алгоритмы RRT и A* в качестве глобальных планировщиков. Для подробного объяснения этих алгоритмов мы отсылаем ч
% итателя к оригинальным статьям, а теперь приступим к обзору.

% Псевдокоды обоих алгоритмов приведены в Alg.~\ref{alg:RRT-POLAMP} и Alg.~\ref{alg:ASTAR-POLAMP} соответственно. Основное различие между алгоритмами 
% на основе случайногосэмплирования (т. е. RRT) и алгоритмами на основе решетки (т. е. A*) заключается в том, как выбрать состояние для расширения и 
% как расширить данное состояние. С одной стороны, RRT использует \textit{RandomSample} в пространстве состояний для случайного роста дерева поиска 
% из \emph{Nearest} состояния в дереве, используя \emph{Extend} для ограничения максимального расстояния между состояниями, которые должны быть соединены. 
% С другой стороны, A* не выбирает случайную выборку, а скорее использует детерминированную приоритетную очередь состояний OPEN, чтобы выбрать, какое состояние 
% расширять (расширять). Очередь OPEN сортируется в порядке возрастания значений $f$, где $f(s) = g(s) + \epsilon \cdot h(s)$ состоит из двух функций стоимостей
% $g(s)$ и $h( с)$. $g(s)$ — стоимость кратчайшего пути от начального состояния до текущего, а $h(s)$ — эвристическая оценка стоимости от $s$ до цели. 
% После выбора наиболее перспективного состояния A* следующие состояния (\emph{Наследники}) используют фиксированный набор примитивов движения, 
% посредством которых робот достигает следующих состояний.

% Основное различие между этими классическими алгоритмами и POLAMP заключается в том, что POLAMP явно учитывает моменты времени, чтобы принять 
% во внимание динамические препятствия при планировании. Локальное планирование реализуется с помощью функции \textit{RL-STEER}. Эта функция решает 
% задачу локального планирования, определяемую двумя состояниями $s_i$ и $s_j$. Если расстояние между $s_i$ и $s_{goal}$ меньше $D$, то цель пытается 
% быть достигнута из $s_i$. Для достижения целевого состояния используется стратегия \textbf{RL-PI}, имеющая доступ к информации о том, как движутся 
% динамические препятствия, т.е. $Obs(t)$. Если \textbf{RL-PI} удалось соединить состояния, он возвращает сгенерированную траекторию $s_j.tr$ и время 
% достижения целевого состояния, т.е. $s_j.t$. Таким образом, все состояния в дереве поиска несут информацию о времени их достижения, которая используется 
% при планировании.

% В данной работе мы используем модифицированную версию RRT, когда на каждой итерации \emph{Nearest} получает несколько $N_{nbs}$ с максимальным радиусом 
% расширения $R_{ext}$ и пытается сгенерировать к ним траектории до тех пор, пока не из них построено.

% В отличие от исходного алгоритма А*, где поиск заканчивается при расширении целевого состояния, в данной работе поиск заканчивается, как только найдена 
% траектория к конечному состоянию. Для генерации преемников мы используем технику онлайн-примитивов движения из~\cite{lin2021search}, т.е. применяем 
% дискретные управления $\xi = (a, \gamma)$ на период $H$ для определения желаемых конфигураций робота. Затем мы используем нашу обученную стратегию для 
% построения траекторий без столкновений к этим конфигурациям.

% % \begin{algorithm}[ht!]
% % \caption{POLAMP with A* planner}
% % \label{alg:ASTAR-POLAMP}
% % \begin{algorithmic}[1]
% %     \Require $s_{start}$, $s_{goal}$, $Obs(t)$, $T$, $\xi$, $D$, RL-PI, $N_{max}$
% %     \Ensure $\mathcal{P}$: Motion Plan
% %     \State CLOSED $\gets \emptyset$, OPEN $\gets \emptyset$
% %     \State $s_{start}.t \gets 0$, $ g(s_{start}) \gets 0$, $f(s_{start}) \gets h(s_{start})$ 
% %     \State OPEN $\gets$ \Call{Insert}{$s_{start}$}
% %     \While{OPEN is not empty or $N_{max}$ was not reached}
% %         \State $s_i \gets $ OPEN.POP(), CLOSED $\gets$ \Call{Insert}{$s_i$}
% %         \State SUCCESSORS $\gets$ \Call{GetNextStates}{$\xi, T$}
% %         \For{$s_j \in$ SUCCESSORS}
% %             %\State \textcolor{blue}{$\triangleright$ $g(s_{t_k})$ is the current time of $s_{t_k}$ in A*}
% %             \State $s_j \gets$ \Call{RL-Steer}{$s_i, s_j, s_{goal}, Obs(t)$, RL-PI, $D$}
% %             \If{$s_j.tr$ is empty}
% %                 \State \textbf{continue}
% %             \EndIf
% %             \If{$s_{goal}.tr$ is not empty}
% %                 \State CLOSED $\gets s_{t_{n}}$, CLOSED $\gets s_{goal}$ 
% %                 \State \Return $\mathcal{P}$ = \Call{MotionPlan}{CLOSED}
% %             \EndIf
% %             \State $c(s_i, s_j) \gets $\Call{COST}{$s_{t_n}.tr$}
% % %            \If {$s_{t_{n}}$ was not visited before}
% % %                \State $g(s_{t_{n}})) \gets \infty$, $f(s_{t_{n}})) \gets \infty$ 
% % %            \EndIf
% %             \If{$g(s_j)$ is better than any previous one}
% %                 \State OPEN $\gets$ \Call{Insert}{$s_j$}
% %             \EndIf
% %             %\If{$g(s_{t_{n}}) > g(s{t_{k}}) + c(s{t_{k}}, s_{t_{n}})$}
% %             %    \State $g(s_{t_{n}}) = g(s{t_{k}}) + c(s{t_{k}}, s_{t_{n}})$
% %             %    \State $f(s_{t_{n}}) = g(s_{t_{n}}) + \epsilon \cdot h(s_{t_{n}})$
% %             %    \State OPEN $\gets$ \Call{Insert}{$s_{t_{n}}$}
% %             %\EndIf
% %         \EndFor
% %     \EndWhile
% %     \State \Return $\mathcal{P} = \emptyset$
% %     %\EndFunction
% % \end{algorithmic}
% % \end{algorithm}


% \section{Experimental Evaluation}

% Мы оценивали POLAMP (и сравнивали его с конкурентами) в двух типах сред: со статическими препятствиями и со статическими и динамическими препятствиями.

% \subsection{Policy learning}

% Для обучения стратегии мы создали набор данных различных задач (начальное и целевое состояния) в трех типах сред: пустое, статическое, динамическое. 
% Каждая из этих сред имела размер $40m \times 40m$. Каждая задача генерировалась случайным образом таким образом, чтобы расстояние между стартовой и 
% целевой локациями находилось в интервале $[15, 30]$m, при этом разница в ориентациях не превышала $\frac{\pi}{4}$ . Задача считалась решенной, если 
% агент достиг целевого состояния с евклидовой ошибкой $\epsilon_{\rho} \leq 0,3$ m и ошибкой ориентации $\epsilon_{\theta} \leq \pi/18$ rad без 
% столкновений. 

% Для генерации задач в статических средах мы выбрали $12$ фрагментов размером $40m \times 40m$ из карты, изображенной на рис.~\Ref{fig:environments} 
% слева (Map1), размер которой составляет $100m \times 100m$. Для обучения в динамичной среде мы наполнили статическую среду одним состязательным 
% динамическим препятствием. Т.е. стартовое состояние динамических препятствий и его траектория генерировались полуслучайно таким образом, что с очень 
% высокой вероятностью оно пересечет путь агента и заставит последнего объехать/ждать. Иллюстрация приведена на рис.~\ref{fig:learningEnvironment}.

% Для обучения в динамичной среде мы наполнили статическую среду одним состязательным динамическим препятствием. Т.е. стартовое состояние динамических 
% препятствий и его траектория генерировались полуслучайно таким образом, что с очень высокой вероятностью оно пересечет путь агента и заставит последнего 
% объехать/ждать. Иллюстрация приведена на рис.~\ref{fig:learningEnvironment}.

% Подобно набору данных поезда, мы создали отдельный набор задач проверки. Мы использовали их для измерения прогресса обучения, т.е. время от времени 
% оценивали производительность обученной в данный момент стратегии на задачах валидации. Если успешность (доля решенных задач) оказалась ниже 80


% %\textbf{The effect of curriculum learning.} To qualitatively assess the effect of the proposed curriculum learning we trained two policies: the first (baseline) was trained immediately in the dynamic environment, $\pi^{stand}$, while the second one, $\pi^{curr}$, was trained with the proposed three-stage curriculum. The corresponding learning curves are shown in Fig~\ref{fig:curriculum-training}. Evidently the curriculum policy $\pi^{curr}$ starts to converge from approx. 300M time step with almost 30 of reward and in this time the standard policy $\pi^{stand}$ only achieves the reward of 13 (and starts converging later). Thus, we confirm that the suggested curriculum leads to a faster convergence, which is especially useful when the resources, e.g. training time, are limited.

% \textbf{Эффект обучения по учебной программе.} Чтобы качественно оценить эффект от обучения по предлагаемой учебной программе, мы обучили две стратегии: 
% первая (базовая) обучалась сразу в динамической среде $\pi^{stand}$, а вторая один, $\pi^{curr}$, прошел обучение по предложенной трехступенчатой 
% программе. Соответствующие кривые обучения показаны на рис~\ref{fig:curriculum-training}. Очевидно, стратегия учебной программы $\pi^{curr}$ начинает 
% сближаться примерно с 2000 года. Временной шаг 300M с вознаграждением почти 30, и за это время стандартная стратегия $\pi^{stand}$ достигает 
% вознаграждения только 13 (и начинает сходиться позже). Таким образом, мы подтверждаем, что предлагаемая учебная программа приводит к более быстрой 
% конвергенции, что особенно полезно, когда ресурсы, например время обучения ограничено.

% % \begin{figure}[!t]
% %     % \centering
% %     % \includegraphics{}
% %     \includegraphics[width=0.5\textwidth]{Images/PPOCurr-reward.png}
% %     %\includegraphics[width=0.5\textwidth]{Images/ValidationSuccessRate.png}
% %     \caption{
% %     A comparison of learning curves between curriculum and standart learning for our policy. The dash lines represent the intermediate trained policy in the respecting environment.}
% %     \label{fig:curriculum-training}
% % \end{figure}

% \begin{table}[ht]
% \begin{center}
% \resizebox{0.6\linewidth}{!}{
% \begin{tabular}{p{0.10\linewidth}|p{0.10\linewidth}p{0.15\linewidth}p{0.10\linewidth}}
% Agent & Dynamic & Orientation & SR \%\\
% \hline \hline
% $\pi^{st}_{w/o-\theta}$ & no & no & 99\\
% $\pi^{st}_{w-\theta}$ & no & yes & 32\\
% $\pi^{dyn}_{w/o-\theta}$ & yes & no & 28\\
% $\pi^{dyn}_{w-\theta}$ & yes & yes & 22\\
% \hline 
% \hline
% \end{tabular}}
% \caption{The results of the trained DDPG agent in different setups.}
% \label{tab:DDPG-agents}
% \end{center}
% \end{table}

% \textbf{Обучение обучаемого базового уровня.} Обучаемый базовый уровень, с которым мы в первую очередь стремились сравнить, был 
% RL-RRT~\cite{Chiang2019RL-RRT}. Как и POLAMP, это комбинация глобального планировщика RRT с обучаемым локальным планировщиком, основанным на стратегии 
% DDPG. Чтобы обеспечить справедливое сравнение, мы обучили эту стратегию на нашем наборе данных с нуля. Однако даже после длительного обучения его 
% успешность выполнения проверочных задач не превышала 22\%.

% Чтобы понять причины такой производительности, мы провели дополнительное обучение трем вариантам этой стратегии на более простых настройках. 
% Характеристики этих установок и полученные в результате показатели успеха показаны в Таблице~\ref{tab:DDPG-agents}. Примечательно, что стратегия, 
% которая игнорировала ограничения ориентации и динамические препятствия (те же настройки, что и RL-RRT), $\pi^{stat}_{w/o-\theta}$, показала очень 
% хорошую производительность — почти 100\% Шанс успеха. Это соответствует оригинальной статье о RL-RRT, поскольку авторы рассматривали этот параметр. 
% Однако когда настройка становится более сложной, производительность стратегии значительно падает. Например, стратегия, игнорирующая динамическое 
% препятствие, $\pi^{stat}_{w/-\theta}$, показала только 32\% SR, а стратегия, игнорирующая ориентацию цели, $\pi^{dyn }_{w/o-\theta}$, -- 28\%. 
% Таким образом, мы полагаем, что этот тип стратегии имеет приемлемую производительность только в базовых настройках.

% Плохая работа RL-RRT в случае более сложных условий внешней среды и при большом количестве динамических препятствий обусловлена, 
% прежде всего, нестабильностью процесса обучения алгоритма DDPG в стохастической среде. DDPG относится к классу внеполитических методов, сохраняет 
% в буфере воспроизведения опыт различных эпизодов (в том числе тех, которые привели к коллизиям) и генерирует детерминированную стратегию относительно 
% функции значения. В POLAMP мы используем метод PPO по политике, когда для улучшения стратегии рассматриваются только самые последние актуальные 
% траектории, которые на более поздних этапах обучения вряд ли будут содержать ситуации столкновения. В ряде работ~\cite{SAC, MPO, Muesli} алгоритмы
%  on-policy показали существенное преимущество перед off-policy в стохастической среде благодаря способности генерировать стохастическую стратегию. 
%  Преимущество ППО перед ДДПГ в нашей задаче неоспоримо при использовании обучения по учебной программе, когда буфер повтора не позволяет DDPG 
%  адаптироваться к новым условиям следующего этапа обучения.

% % \begin{figure*}[t]
% %     \centering
% %     \includegraphics[width=0.31\linewidth]{Images/DynSuccessRate.png}
% %     \includegraphics[width=0.31\linewidth]{Images/DynTimeToReach.png}
% %     \includegraphics[width=0.31\linewidth]{Images/DynSamples.png}
% %     \caption{Planning results for the maps with dynamic obstacles (success rate, time to reach and number of samples). The legend for all algorithms is shown in the figure on the right.}
% %     \label{fig:MetricsOnDynamicMaps}
% % \end{figure*}

% \begin{table}[t]
% \begin{center}
% \resizebox{0.9\linewidth}{!}{
% \begin{tabular}{p{0.05\linewidth}|p{0.25\linewidth}p{0.08\linewidth}p{0.08\linewidth}p{0.14\linewidth}p{0.11\linewidth}}
 
%  Map & Planner & SR,\% & TTR,\% & Samples,\% & Time,\%\\
%  \hline \hline
%   \multirow{2}{4em}{1} & POLAMP-RRT & 100 & 100 & 100 & 100\\
%   & POLAMP-A* & 100 & 93 & 179 & 103\\
%   & RRT-ES & 90 & 120 & 3851 & 104\\
%   & RL-RRT & 40 & 96 & 2424 & 578\\
%   & SST* & 85 & 140 & 4124 & 111\\
%  \hline 
%   \multirow{2}{4em}{2} & POLAMP-RRT & 100 & 100 & 100 & 100\\
%   & POLAMP-A* & 100 & 78 & 121 & 85\\
%   & RRT-ES & 62.5 & 143 & 1322 & 107\\
%   & RL-RRT & 4.5 & 153 & 677 & 308\\
%   & SST* & 82.5 & 123 & 1225 & 102\\
%  \hline
%   \multirow{2}{4em}{3} & POLAMP-RRT & 100 & 100 & 100 & 100\\
%   & POLAMP-A* & 100 & 84 & 143 & 89\\
%   & RRT-ES & 31 & 102 & 3426 & 98\\
%   & RL-RRT & 8 & 126 & 1532 & 407\\
%   & SST* & 58.8 & 141 & 3560 & 101\\
%  \hline
%  \hline
% \end{tabular}
% }
% \caption{Results of the experiments on the static maps.}
% \label{tab:resultsForStaticEnvironment}
% \end{center}
% \end{table}

% \subsection{Evaluation In Static Environments}

% Для оценки мы использовали три разные карты, напоминающие парковки — см. Рис.~\ref{fig:environments}. Каждая карта имела размер $100 m \times 60 m$ и 
% была создана на основе набора данных из ~\cite{parkingDataSet}. Обратите внимание, что при обучении политикой наблюдались только несколько фрагментов 
% Map1, а Map2 и Map3 при обучении вообще не использовались. Для каждой карты мы создали 20 различных экземпляров планирования, т. е. пар местоположений 
% «старт-цель». Мы генерировали их случайным образом и отбрасывали экземпляры, для которых расстояние по прямой между стартом и целью было менее $50$ m 
% (во избежание несложных задач). Ориентации старта и цели также выбирались случайным образом как мультипликатив $90^\circ$. Каждый тест повторялся 30 раз. Тест считался неудачным, если робот не мог достичь цели со следующими допусками: $\epsilon_{\rho} \leq 0,5$ м и $\epsilon_{\theta} \leq \pi / 18$.

% Мы сравнили POLAMP со следующими алгоритмами: RRT, в котором использовалась известная необучаемая функция управления, основанная на экспоненциальной 
% стабилизации~\cite{Astolfi1999ESPOSQ} (обозначаемая RRT-ES), кинодинамический планировщик движения SST*~\cite{li2016asymptotically} , 
% RL-RRT~\cite{Chiang2019RL-RRT} — современный метод планирования с обучаемым локальным планировщиком (подробности об изучении этого планировщика 
% были представлены выше).

% Для RRT-части алгоритмов задаём радиус метода Extend $R_{ext} = 10$m, максимальное расстояние, с которого мы можем достичь цели $D = 30$m, 
% количество ближайших соседей $ N_{nbs} = 5$ и максимальное количество итераций РРТ $N_{max} = 1500$ для ПОЛАМПА-РРТ и РЛ-РРТ и $N_{max} = 3000$ для 
% РРТ-ЭС и ССТ *. Для ПОЛАМПА-А* мы использовали те же параметры, что и для РРТ. Кроме того, для создания решетки мы использовали 7 дискретных углов 
% поворота, равномерно расположенных в диапазоне $[\gamma_{min}, \gamma_{max}]$, линейной скорости $v=2$ и временного горизонта $H = 3$ с. примитивов 
% движения. Все эти значения были выбраны после предварительной оценки, направленной на определение подходящих значений параметров.

% Мы использовали следующие показатели: показатель успеха (SR) — как часто планировщик создает путь, ведущий к цели, время достижения цели (TTR), общее 
% количество выборок и время выполнения алгоритма.

% Результаты представлены в таблице~\ref{tab:resultsForStaticEnvironment}. Примечательно, что POLAMP имеет гораздо более высокий уровень успеха по 
% сравнению с другими алгоритмами, достигая почти 100\% на каждой карте. Это показывает, что наш обучаемый локальный планировщик действительно хорошо 
% обобщает невидимые условия. Наблюдаемая тенденция заключается в том, что POLAMP требует гораздо меньше выборок, чем RRT-ES, для создания плана движения. 
% Например, для Map2 POLAMP требуется в 14 и 12 раз меньше образцов по сравнению с RRT-ES и SST* соответственно. Это связано с тем, что POLAMP выполняет 
% предотвращение столкновений для локального рулевого управления, а RRT-ES и SST* этого не делают. По сравнению с RL-RRT, POLAMP также требует меньше 
% образцов.

% Также можно отметить, что RL-RRT имеет более высокий уровень успеха для Карты 1, чем для других карт, а это означает, что, в отличие от нашей стратегии, 
% стратегия RL-RRT не очень хорошо обобщается на две другие карты. Мы можем предположить, что основная причина того, что RL-RRT не может хорошо работать 
% на Map2 и Map3, заключается в том, что обучаемый компонент этого планировщика, то есть DDPG, не смог достаточно хорошо обучаться в нашей настройке, то 
% есть ему были предоставлены только экземпляры. которые были взяты из Map1. Другими словами, стратегия DDPG не смогла хорошо обучиться в нашем наборе 
% данных и была заменена на Map1. Между тем, PPO, который использовал тот же объем данных для обучения, смог обобщить решение локальных запросов поиска 
% пути на (невидимых во время обучения) Map2 и Map3. Таким образом, мы делаем вывод, что PPO является более эффективной политикой выборки, которой, как 
% правило, следует отдавать предпочтение перед DDPG в аналогичных конфигурациях.

% \subsection{Evaluation In Dynamic Environments}


% Для этой серии экспериментов мы использовали Map2 и Map3, т.е. карты, которые не использовались для обучения. На этих картах было разное количество 
% динамических препятствий: от $0$ до $70$. Каждое динамическое препятствие представляет собой робота прямоугольной формы, напоминающего автомобиль. 
% Его траектория генерируется путем выборки случайного управляющего сигнала $(a, \omega)$ каждый 10-й временной шаг. Мы сгенерировали 5 различных 
% траекторий для каждого динамического препятствия. Для каждой карты были выбраны две разные пары «старт-гол». Для планировщиков, основанных на выборке, 
% каждый тест повторялся 20 раз.

% Как и прежде, мы сравнивали POLAMP с RL-RRT. Мы также сравнили его с A*-CMP~\cite{lin2021search}. Для этого алгоритма мы использовали те же параметры, 
% что и для POLAMP-A*. Другим базовым вариантом стала комбинация RRT с оригинальным подходом динамического окна (DWA)~\cite{DWA} в качестве локального 
% планировщика (RRT-DWA). Последний способен избегать движущихся препятствий и широко используется в робототехнике. Для RRT-DWA мы не учитывали 
% окончательную ориентацию, поскольку DWA не приспособлен подчиняться ограничениям ориентации. Кроме того, мы сравнили RRTX~\cite{Otte2016RRTX}, 
% в котором использовалась функция управления Дубинса~\cite{DUBINS}. Этот алгоритм, по сути, представляет собой алгоритм типа 
% «план-выполнение-перепланирование», который повторно использует дерево поиска, пока робот движется к цели. Для повышения производительности RRTX на 
% каждой итерации перепланирования мы не учитывали движущиеся препятствия, расположенные на расстоянии более 20 м от робота. В случае RRTX SR означает, 
% как часто робот может достигать цели без столкновений при прохождении пути. Кроме того, поскольку RRTX требуется гораздо больше выборок во время 
% перепланирования, мы не показываем эту метрику для RRTX.

% Результаты представлены на рис~\ref{fig:MetricsOnDynamicMaps}. Первая явная тенденция заключается в том, что POLAMP-RRT, POLAMP-A* и A*-CMP
% во всех случаях поддерживать высокий уровень успеха ($>92\%$) до тех пор, пока количество динамических препятствий не превысит 50. Однако POLAMP-A* и 
% POLAMP-RRT требуют гораздо меньше выборок, чем A*-CMP, для определения траектории. Это связано с тем, что A*-CMP требует двух групп примитивов. Одна 
% группа примитивов позволяет ускоряться и двигаться с постоянной скоростью, а другая группа пытается замедлиться, чтобы избежать столкновения с 
% динамическими препятствиями. Однако нашему алгоритму требуется только одна группа примитивов, поскольку наша стратегия способна замедляться, чтобы 
% избежать столкновения с динамическими препятствиями, когда это необходимо.

% Мы также отмечаем, что существует компромисс между POLAMP-RRT, POLAMP-A* и A*-CMP. С одной стороны, POLAMP-RRT немного лучше, чем базовый A*-CMP и 
% наш POLAMP-A* с точки зрения успеха. Благодаря случайности RRT, POLAMP-RRT способен исследовать больше и решать сложные задачи, в отличие от A*, 
% который выполняет систематический неисследовательский поиск. С другой стороны, A*-CMP имеет наименьшую длительность по сравнению с остальными алгоритмами. 
% Последнее связано с тем, что на каждой итерации A*-CMP использует минимальное и максимальное ускорение для генерации соседей, т. е. алгоритм делает 
% резкие изменения скорости. Однако наш местный обучаемый руль старается плавно менять скорость из-за наличия препятствий. Наш алгоритм лучше, чем другие 
% базовые версии RL-RRT и RRT-DWA. Из-за плохой производительности $\pi^{dyn}_{w-\theta}$ алгоритм RL-RRT не показал хороших результатов. RRT-DWA хорошо 
% работает только тогда, когда количество препятствий невелико. 

% POLAMP-RRT и POLAMP-A* также лучше, чем RRTX. RRTX пытается перепланировать путь в режиме онлайн, но иногда, когда текущий путь перекрыт динамическими 
% препятствиями, робот вынужден остановиться и оставаться на своем месте, пока не найдет другое решение. В таких ситуациях робот может попасть в тупик, 
% из которого невозможно выбраться без столкновения из-за движущихся препятствий. Эта проблема связана с тем, что RRTX не учитывает будущие траектории 
% движения динамических препятствий при планировании. Кроме того, TTR RRTX почти вдвое больше, чем у других алгоритмов. Это связано с тем, что RRTX имеет 
% резкие изменения пути, когда на путь влияет появление динамических препятствий.

% В целом, проведенные эксперименты показывают, что наша стратегия $\pi^{curr}$ хорошо обобщается как на новые среды, так и на увеличивающееся количество 
% динамических препятствий (напомним, что она обучалась только с одним движущимся препятствием). Сочетание этой стратегии с глобальным планированием 
% на основе поиска или выборки хорошо работает в сложных условиях с десятками одновременно движущихся препятствий. Некоторые экспериментальные видеоролики 
% представлены в Мультимедийных материалах.

% \section{Conclusion}


% В данной статье мы рассмотрели задачу кинодинамического планирования неголономного робота в средах с динамическими препятствиями. 
% Мы усовершенствовали два классических метода планирования, A* и RRT, добавив обучаемую функцию рулевого управления, которая учитывает кинодинамические
%  ограничения, а также статические и движущиеся препятствия. Мы разработали функцию вознаграждения и создали специальную программу обучения поведению 
%  рулевого управления. Полученный алгоритм POLAMP был оценен эмпирически как в статических, так и в динамических средах и показал, что он превосходит 
%  современные базовые модели (как обучаемые, так и необучаемые).

\section{Эксперименты}\label{sec:ch3/sec3}