@Patent{patbib1,
  heading =      {Заявка 1234567 Рос. федерация, МПК\ensuremath{^7} B 64 G 1/00},
  author =       {Иванов, И. И.},
  authortype =   {Российская Федерация},
  title =        {Многоразовая ракета-носитель},
  media =        {text},
  holder =       {заявитель Технологические технологии},
  credits =      {патент. поверенный Егорова Г. Б.},
  reqnumber =    {2000108705/28},
  date =         {2020-01-03},
  publdate =     {2020-01-02},
  publication =  {Бюл. № 7 (I ч.)\midsentence},
  prdate =       {2020-01-01},
  prnumber =     {09/289, 037},
  prcountry =    {countryru},
  pagetotal =    {5 с.~: ил.},
  language =     {russian},
  authorpatent = {yes},
}

@Patent{progbib1,
  heading =      {Свидетельство о гос. регистрации программы для {ЭВМ}},
  author =       {Петров, П. П.},
  title =        {foobar},
  media =        {text},
  holder =       {НИИ~ГДААДАВБА},
  reqnumber =    1234567890,
  publdate =     {2020-01-02},
  date =         {2020-01-01},
  prnumber =     1234567890,
  prcountry =    {countryru},
  language =     {russian},
  authorprogram ={yes},
}

@article{RRT,
  title={Rapidly-exploring random trees: A new tool for path planning},
  author={LaValle, Steven M"},
  year={1998},
  publisher={Citeseer},
}
%3
@article{SAMPLING,
  title={Sampling-based algorithms for optimal motion planning},
  author={Karaman, Sertac and Frazzoli, Emilio},
  journal={The international journal of robotics research},
  volume={30},
  number={7},
  pages={846--894},
  year={2011},
  publisher={Sage Publications Sage UK: London, England}
}
%4
@article{THETA*,
  title={Theta*: Any-angle path planning on grids},
  author={Daniel, Kenny and Nash, Alex and Koenig, Sven and Felner, Ariel},
  journal={Journal of Artificial Intelligence Research},
  volume={39},
  pages={533--579},
  year={2010}
}
%5
@article{OMPL,
  author={I. A. {Sucan} and M. {Moll} and L. E. {Kavraki}},
  journal={IEEE Robotics   Automation Magazine}, 
  title={The Open Motion Planning Library}, 
  year={2012},
  volume={19},
  number={4},
  pages={72-82},
  doi={10.1109/MRA.2012.2205651}}
%6
@article{KINOD,
  title={Randomized kinodynamic planning},
  author={LaValle, Steven M and Kuffner Jr, James J},
  journal={The international journal of robotics research},
  volume={20},
  number={5},
  pages={378--400},
  year={2001},
  publisher={SAGE Publications}
}
%7
@inproceedings{POSQ,
  author={L. {Palmieri} and K. O. {Arras}},
  booktitle={Proceedings of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={A novel {RRT} extend function for efficient and smooth mobile robot motion planning}, 
  year={2014},
  pages={205-211}
}
%8
@article{RS,
  title={Optimal paths for a car that goes both forwards and backwards},
  author={Reeds, James and Shepp, Lawrence},
  journal={Pacific journal of mathematics},
  volume={145},
  number={2},
  pages={367--393},
  year={1990},
  publisher={Mathematical Sciences Publishers}
}
%9
@article{DUBINS1,
  title={On curves of minimal length with a constraint on average curvature, and with prescribed initial and terminal positions and tangents},
  author={Dubins, Lester E},
  journal={American Journal of mathematics},
  volume={79},
  number={3},
  pages={497--516},
  year={1957},
  publisher={JSTOR}
}
%10
@article{DUBINS,
  title={On curves of minimal length with a constraint on average curvature, and with prescribed initial and terminal positions and tangents},
  author={Dubins, Lester E},
  journal={American Journal of mathematics},
  volume={79},
  number={3},
  pages={497--516},
  year={1957},
  publisher={JSTOR}
}
%11
@inproceedings{BIT*,
  title={Batch informed trees (BIT*): Sampling-based optimal planning via the heuristically guided search of implicit random geometric graphs},
  author={Gammell, Jonathan D and Srinivasa, Siddhartha S and Barfoot, Timothy D},
  booktitle={2015 IEEE international conference on robotics and automation (ICRA)},
  pages={3067--3074},
  year={2015},
  organization={IEEE}
}
%12
@inproceedings{THETA*RRT,
  title={{RRT}-based nonholonomic motion planning using any-angle path biasing},
  author={Palmieri, Luigi and Koenig, Sven and Arras, Kai O},
  booktitle={Proceedings of the 2016 IEEE International Conference on Robotics and Automation},
  pages={2775--2781},
  year={2016}
}
%13
@article{A*,
  title={A formal basis for the heuristic determination of minimum cost paths},
  author={Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal={IEEE transactions on Systems Science and Cybernetics},
  volume={4},
  number={2},
  pages={100--107},
  year={1968},
  publisher={IEEE}
}
%14
@inproceedings{OPTIMAL,
  title={An {NMPC} approach using convex inner approximations for online motion planning with guaranteed collision avoidance},
  author={Schoels, Tobias and Palmieri, Luigi and Arras, Kai O and Diehl, Moritz},
  booktitle={Proceedings of the 2020 IEEE International Conference on Robotics and Automation},
  pages={3574--3580},
  year={2020}
}
%15
@inproceedings{POSQ1,
  title={Distance metric learning for {RRT}-based motion planning for wheeled mobile robots},
  author={Palmieri, Luigi and Arras, Kai O},
  booktitle={Proceedings of the 2015 IEEE International Conference on Robotics and Automation},
  pages={637-643},
  year={2015}
}

@article{experimentalBenchmark,
  author =		 {Eric H., Luigi P. , Kai O. , Gaurav S. and Sven K.},
  title = 		 {Experimental Comparasion of Global Motion Planning Algorithms for Wheeled Mobile Robots},
  publisher = 	 {arXiv, Preprint},
  year = 		 {2020}
}

@inproceedings{BIT*SQP,
  author =		 {Xie, C. and Van Den, Berg J and Patil, S. and Abbeel, P.},
  title = 		 {Toward asymptotically optimal motion planning for kinodynamic systems using a two-point boundary value problem solver},
  publisher = 	 {In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  year = 		 {2015}
}

@article{Benchmarks,
  title={Benchmarks for Grid-Based Pathfinding},
  author={Sturtevant, N.},
  journal={Transactions on Computational Intelligence and AI in Games},
  volume={4},
  number={2},
  pages={144 -- 148},
  year={2012}
}
}


@inproceedings{JPS,
  title={Online Graph Pruning for Pathfinding On Grid Maps},
  author={Harabor, D. and Grastien, A.},
  booktitle={Proceedings of the 25th AAAI Conference on Artificial Intelligence},
  pages={1114--1119},
  year={2011}
}

@inproceedings{RRT*,
author = {Karaman, Sertac and Walter, Matthew and Perez, Alejandro and Frazzoli, Emilio and Teller, Seth},
year = {2011},
pages = {1478--1483},
title = {Anytime Motion Planning using the {RRT*}},
booktitle = {Proceedings of the 2011 IEEE International Conference on Robotics and Automation}
}

@article{MotionPrimitives,
  title={Planning long dynamically feasible maneuvers for autonomous vehicles},
  author={Likhachev, Maxim and Ferguson, Dave},
  journal={The International Journal of Robotics Research},
  volume={28},
  number={8},
  pages={933--945},
  year={2009}
}

@inproceedings{ROS,
author = {Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew},
year = {2009},
title = {{ROS}: an open-source Robot Operating System},
journal = {Proceedings of the Third ICRA Workshop on Open Source Software}
}

@inproceedings{andreychuk2018elian,
  title={{eLIAN}: Enhanced algorithm for angle-constrained path finding},
  author={Andreychuk, Anton and Soboleva, Natalia and Yakovlev, Konstantin},
  booktitle={Proceedings of the Russian Conference on Artificial Intelligence},
  pages={206--217},
  year={2018}
}

@article{Staroverov2020RealTimeON,
  title={Real-Time Lidar-based Localization of Mobile Ground Robot},
  author={I. Belkin and A. Abramenko and D. Yudin},
  journal={Procedia Computer Science},
  pages={in press},
  year={2021}
}

@article{Gammell2020ABIT*,
  author={Strub, Marlin P. and Gammell, Jonathan D.},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Advanced BIT* (ABIT*): Sampling-Based Planning with Advanced Graph-Search Techniques}, 
  year={2020}
}

@article{RRT*MotionPrimitives,
  author={Sakcak, B. and Bascetta, L. and Ferretti, G.},
  title={Sampling-based optimal kinodynamic planning with motion primitives}, 
  journal={Auton Robot 43},
  pages={1715–1732},
  year={2019}
}

@book{Corke2013Robotics,
    author = {Corke, Peter},
    title = {Robotics, Vision and Control: Fundamental Algorithms in MATLAB},
    year = {2013},
    isbn = {3642201431},
    publisher = {Springer Publishing Company}
}

@article{gonzalez2015review,
  title={A review of motion planning techniques for automated vehicles},
  author={Gonz{\'a}lez, David and P{\'e}rez, Joshu{\'e} and Milan{\'e}s, Vicente and Nashashibi, Fawzi},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  pages={1135--1145},
  year={2015}
}
  
@inproceedings{rufli2010design,
  title={On the design of deformable input-/state-lattice graphs},
  author={Rufli, Martin and Siegwart, Roland},
  booktitle={2010 IEEE International Conference on Robotics and Automation},
  pages={3071--3077},
  year={2010},
}

@inproceedings{ziegler2009spatiotemporal,
  title={Spatiotemporal state lattices for fast trajectory planning in dynamic on-road driving scenarios},
  author={Ziegler, Julius and Stiller, Christoph},
  booktitle={2009 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={1879--1884},
  year={2009},
}

@article{kuwata2009real,
  title={Real-time motion planning with applications to autonomous urban driving},
  author={Kuwata, Yoshiaki and Teo, Justin and Fiore, Gaston and Karaman, Sertac and Frazzoli, Emilio and How, Jonathan P},
  journal={IEEE Transactions on control systems technology},
  volume={17},
  number={5},
  pages={1105--1118},
  year={2009}
}

@article{hsu2002randomized,
  title={Randomized kinodynamic motion planning with moving obstacles},
  author={Hsu, David and Kindel, Robert and Latombe, Jean-Claude and Rock, Stephen},
  journal={The International Journal of Robotics Research},
  volume={21},
  number={3},
  pages={233--255},
  year={2002}
}

@inproceedings{hwan2011anytime,
  title={Anytime computation of time-optimal off-road vehicle maneuvers using the RRT},
  author={hwan Jeon, Jeong and Karaman, Sertac and Frazzoli, Emilio},
  booktitle={2011 50th IEEE Conference on Decision and Control and European Control Conference},
  pages={3276--3282},
  year={2011}
}

@inproceedings{webb2013kinodynamic,
  title={Kinodynamic {RRT*}: {A}symptotically optimal motion planning for robots with linear dynamics},
  author={Webb, Dustin J and Van Den Berg, Jur},
  booktitle={Proceedings of the 2013 IEEE International Conference on Robotics and Automation ({ICRA} 2013)},
  pages={5054--5061},
  year={2013}
}

@article{Chiang2019RL-RRT,
   author = {Hao Tien Lewis Chiang and Jasmine Hsu and Marek Fiser and Lydia Tapia and Aleksandra Faust},
   doi = {10.1109/LRA.2019.2931199},
   issn = {23773766},
   issue = {4},
   journal = {IEEE Robotics and Automation Letters},
   title = {RL-RRT: Kinodynamic Motion Planning via Learning Reachability Estimators from RL Policies},
   volume = {4},
   year = {2019},
}

@article{Astolfi1999ESPOSQ,
    author = {Astolfi, A.},
    title = {Exponential Stabilization of a Wheeled Mobile Robot Via Discontinuous Control},
    journal = {Journal of Dynamic Systems, Measurement, and Control},
    volume = {121},
    number = {1},
    pages = {121-126},
    year = {1999},
}

@article{Gottschalk1996Separating,
    author = {Gottschalk, S.},
    title = {Separating axis theorem},
    journal = {Department of Computer Science, UNC Chapel Hill},
    volume = {Tech. Rep. TR96-024},
    number = {1},
    year = {1996},
}

@INPROCEEDINGS{Faust2018PRM-RL,
  author={Faust, Aleksandra and Oslund, Kenneth and Ramirez, Oscar and Francis, Anthony and Tapia, Lydia and Fiser, Marek and Davidson, James},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-Based Planning}, 
  year={2018},
  pages={5113-5120},
}

@article{PPO,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  eprinttype = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{TRPO,
  author    = {John Schulman and
               Sergey Levine and
               Pieter Abbeel and
               Michael I. Jordan and
               Philipp Moritz},
  editor    = {Francis R. Bach and
               David M. Blei},
  title     = {Trust Region Policy Optimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning,
               {ICML} 2015, Lille, France, 6-11 July 2015},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {37},
  pages     = {1889--1897},
  publisher = {JMLR.org},
  year      = {2015},
}

@article{Lin2020SearchBasedOT,
  title={Search-Based Online Trajectory Planning for Car-like Robots in Highly Dynamic Environments},
  author={Jiahui Lin and Tong Zhou and Delong Zhu and Jianbang Liu and M. Meng},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.03664}
}

@article{vailland2021CubicBezier,
  title = {{Cubic Bezier Local Path Planner for Non-holonomic Feasible and Comfortable Path Generation}},
  author = {Vailland, Guillaume and Gouranton, Val{\'e}rie and Babel, Marie},
  journal = {{IEEE}},
  pages = {7894-7900},
  year = {2021},
  month = May,
}
 
@article{Otte2016RRTX,
author = {Michael Otte and Emilio Frazzoli},
title ={RRTX: Asymptotically optimal single-query sampling-based motion planning with quick replanning},
journal = {The International Journal of Robotics Research},
volume = {35},
number = {7},
pages = {797-822},
year = {2016},
}

@article{Chen2019Horizon,
author = {Y. Chen, Z. He and S. Li},
title ={Horizon-based lazy optimal RRT for fast, efficient replanning in dynamic environment},
journal = {Auton Robot},
volume = {43},
pages = {2271–2292},
year = {2019},
}

@article{TD3,
   author = {Scott Fujimoto and Herke van Hoof and David Meger},
   month = {2},
   title = {Addressing Function Approximation Error in Actor-Critic Methods},
   url = {http://arxiv.org/abs/1802.09477},
   year = {2018},
}

@article{surveyMotionPlanning,
  author={Paden, Brian and Čáp, Michal and Yong, Sze Zheng and Yershov, Dmitry and Frazzoli, Emilio},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={A Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles}, 
  year={2016},
  volume={1},
  number={1},
  pages={33-55}
  }

@inproceedings{SAC,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
booktitle = {Proceedings of the 35th International Conference on Machine Learning, PMLR},
eprint = {1801.01290},
pages = {1861--1870},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
volume = {80},
year = {2018}
}

@inproceedings{MPO,
abstract = {We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.},
archivePrefix = {arXiv},
arxivId = {1806.06920},
author = {Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
booktitle = {6th International Conference on Learning Representations},
eprint = {1806.06920},
title = {{Maximum a posteriori policy optimisation}},
year = {2018}
}

@inproceedings{Muesli,
abstract = {We propose a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. The update (henceforth Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli does so without using deep search: it acts directly with a policy network and has computation speed comparable to model-free baselines. The Atari results are complemented by extensive ablations, and by additional results on continuous control and 9x9 Go.},
archivePrefix = {arXiv},
arxivId = {2104.06159},
author = {Hessel, Matteo and Danihelka, Ivo and Viola, Fabio and Guez, Arthur and Schmitt, Simon and Sifre, Laurent and Weber, Theophane and Silver, David and van Hasselt, Hado},
booktitle = {Proceedings of the 38th International Conference on Machine Learning, PMLR},
eprint = {2104.06159},
pages = {4214--4226},
title = {{Muesli: Combining Improvements in Policy Optimization}},
url = {http://arxiv.org/abs/2104.06159 http://proceedings.mlr.press/v139/hessel21a.html},
volume = {139},
year = {2021}
}

@article{parkingDataSet,
  author={Hsieh, Meng-Ru and Lin, Yen-Liang and Hsu, Winston H.},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Drone-Based Object Counting by Spatially Regularized Regional Proposal Network}, 
  year={2017},
  pages={4165-4173}
 }
 
 @ARTICLE{DWA,
  author={Fox, D. and Burgard, W. and Thrun, S.},
  journal={IEEE Robotics   Automation Magazine}, 
  title={The dynamic window approach to collision avoidance}, 
  year={1997},
  volume={4},
  number={1},
  pages={23-33}
  }

@inproceedings{lin2021search,
  title={Search-Based Online Trajectory Planning for Car-like Robots in Highly Dynamic Environments},
  author={Lin, Jiahui and Zhou, Tong and Zhu, Delong and Liu, Jianbang and Meng, Max Q-H},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={8151--8157},
  year={2021}
}

@article{hart1968formal,
  title={A formal basis for the heuristic determination of minimum cost paths},
  author={Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal={IEEE transactions on Systems Science and Cybernetics},
  volume={4},
  number={2},
  pages={100--107},
  year={1968},
  publisher={IEEE}
}

@inproceedings{perez2021robot,
  title={Robot navigation in constrained pedestrian environments using reinforcement learning},
  author={P{\'e}rez-D’Arpino, Claudia and Liu, Can and Goebel, Patrick and Mart{\'\i}n-Mart{\'\i}n, Roberto and Savarese, Silvio},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1140--1146},
  year={2021},
  organization={IEEE}
}

@article{kontoudis2019kinodynamic,
  title={Kinodynamic motion planning with continuous-time Q-learning: An online, model-free, and safe navigation framework},
  author={Kontoudis, George P and Vamvoudakis, Kyriakos G},
  journal={IEEE transactions on neural networks and learning systems},
  volume={30},
  number={12},
  pages={3803--3817},
  year={2019},
  publisher={IEEE}
}

@article{li2016asymptotically,
  title={Asymptotically optimal sampling-based kinodynamic planning},
  author={Li, Yanbo and Littlefield, Zakary and Bekris, Kostas E},
  journal={The International Journal of Robotics Research},
  volume={35},
  number={5},
  pages={528--564},
  year={2016},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{phillips2011sipp,
  title={Sipp: Safe interval path planning for dynamic environments},
  author={Phillips, Mike and Likhachev, Maxim},
  booktitle={2011 IEEE International Conference on Robotics and Automation},
  pages={5628--5635},
  year={2011}
}

@inproceedings{kontoudis2020online,
  title={Online, model-free motion planning in dynamic environments: An intermittent, finite horizon approach with continuous-time Q-learning},
  author={Kontoudis, George P and Xu, Zirui and Vamvoudakis, Kyriakos G},
  booktitle={2020 American Control Conference (ACC)},
  pages={3873--3878},
  year={2020}
}

@article{everett2021collision,
  title={Collision avoidance in pedestrian-rich environments with deep reinforcement learning},
  author={Everett, Michael and Chen, Yu Fan and How, Jonathan P},
  journal={IEEE Access},
  volume={9},
  pages={10357--10377},
  year={2021}
}

@article{li2021mpc,
  title={MPC-MPNet: Model-predictive motion planning networks for fast, near-optimal planning under kinodynamic constraints},
  author={Li, Linjun and Miao, Yinglong and Qureshi, Ahmed H and Yip, Michael C},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={3},
  pages={4496--4503},
  year={2021}
}

@article{williams2022trajectory,
  title={Trajectory Planning with Deep Reinforcement Learning in High-Level Action Spaces},
  author={Williams, Kyle R and Schlossman, Rachel and Whitten, Daniel and Ingram, Joe and Musuvathy, Srideep and Pagan, James and Williams, Kyle A and Green, Sam and Patel, Anirudh and Mazumdar, Anirban and others},
  journal={IEEE Transactions on Aerospace and Electronic Systems},
  year={2022}
}

@incollection{Ivanov2021,
abstract = {Local planner makes a trajectory physically executable for an agent. Open Space Planner of Apollo framework based on nonlinear optimization methods smooths the trajectory received from a global planner. Such dependency on a global planner forces an agent to relaunch both planners when local changes occur (e.g., when an environment has dynamic obstacles), what can waste too much time. In this article, we consider a different approach which is based on reinforcement learning. This method allows agent generate a trajectory using information about environment (the current and goal state, lidar sensors, etc.). Experiments conducted on the simplified environment show that such algorithm can be implemented as the local planner in Apollo infrastructure.},
author = {Ivanov, Dmitriy and Panov, Aleksandr I.},
booktitle = {Proceedings of the Fifth International Scientific Conference “Intelligent Information Technologies for Industry” (IITI'21). IITI 2021. Lecture Notes in Networks and Systems},
doi = {10.1007/978-3-030-87178-9_4},
editor = {Kovalev, Sergey and Tarassov, Valery and Snasel, Vaclav and Sukhanov, Andrey},
pages = {35--43},
publisher = {Springer},
title = {{Application of Reinforcement Learning in Open Space Planner for Apollo Auto}},
volume = {330},
year = {2022}
}

@incollection{Gorbov2022,
abstract = {This paper addresses the autonomous parking for a vehicle in environments with static and dynamic obstacles. Although parking maneuvering has reached the level of fully automated valet parking, there are still many challenges to realize the parking motion planning in the presence of dynamic obstacles. One of the most famous autonomous driving platforms is the Baidu Apollo platform. In the Apollo platform, this problem is solved using the classic method hybrid A*. However, this method has two main downsides. Firstly, it generates in some parking scenarios, trajectories that consist of many partitions with different gear types and sizes. Such trajectories are intractable by a self-driving car when testing the Apollo planner on more realistic data coming from a simulator such as SVL. Secondly, the built-in algorithm does not have the ability to interact with dynamic obstacles, which might lead to a collision in some critical parking scenarios. To overcome these issues, we proposed a method based on reinforcement learning, which uses the RL-policy (from POLAMP) allowing us to take into account the kinematic constraints of the vehicle, static and dynamic obstacles. The proposed method was fully integrated into the Apollo platform with developed Cyber RT nodes, which were used for publishing the parking trajectory from our algorithm to the SVL simulator through a ROS/Cyber bridge. The final model demonstrates transferability to the previously unseen experimental environments and flexibility with respect to built-in hybrid A*.},
author = {Gorbov, Gregory and Jamal, Mais and Panov, Aleksandr I.},
booktitle = {Proceedings of the Sixth International Scientific Conference “Intelligent Information Technologies for Industry” (IITI'22). IITI 2022. Lecture Notes in Networks and Systems},
doi = {10.1007/978-3-031-19620-1_27},
editor = {Kovalev, Sergey and Sukhanov, Andrey and Akperov, Imran and Ozdemir, Sebnem},
pages = {283--292},
title = {{Learning Adaptive Parking Maneuvers for Self-driving Cars}},
volume = {566},
year = {2023}
}

@article{Panov2022,
abstract = {The tasks of behavior planning and decision-making learning in a dynamic environment are usually divided and considered separately in control systems for intelligent agents. A new unified hierarchical formulation of the problem of simultaneous learning and planning (SLAP) is proposed in the context of object-oriented reinforcement learning, and an architecture of a cognitive agent that solves this problem is described. A new algorithm for learning actions in a partially observed external environment is proposed using a reward signal, an object-oriented subject description of the states of the external environment, and dynamically updated action plans. The main properties and advantages of the proposed algorithm are considered, including the lack of a fixed cognitive cycle necessitating the separation of planning and learning subsystems in earlier algorithms and the ability to construct and update the model of interaction with the environment, thus increasing the learning efficiency. A theoretical justification of some provisions of this approach is given, a model example is proposed, and the principle of operation of a SLAP agent when driving an unmanned vehicle is demonstrated.},
author = {Panov, Aleksandr I.},
doi = {10.1134/S0005117922060054},
number = {6},
pages = {869--883},
title = {{Simultaneous Learning and Planning in a Hierarchical Control System for a Cognitive Agent}},
volume = {83},
year = {2022}
}

@incollection{Jamal2021,
abstract = {In safety-critical systems such as autonomous driving sys- tems, behavior planning is a significant challenge. The presence of numerous dynamic obstacles makes the driving environment unpredictable. The planning algorithm should be safe, reactive, and adaptable to environmental changes. The paper presents an adaptive maneuver planning algorithm based on an evolving behavior tree created with genetic programming. In addition, we make a technical contribution to the Baidu Apollo autonomous driving platform, allowing the platform to test and develop overtaking maneuver planning algorithms.},
author = {Jamal, Mais and Panov, Aleksandr},
booktitle = {Artificial Intelligence XXXVIII. SGAI 2021. Lecture Notes in Computer Science},
doi = {10.1007/978-3-030-91100-3_26},
editor = {Bramer, Max and Ellis, Richard},
pages = {327--340},
title = {{Adaptive Maneuver Planning for Autonomous Vehicles Using Behavior Tree on Apollo Platform}},
volume = {13101},
year = {2021}
}

